%% train_EREV_agent.m
% EREV RL í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢…)
% íŠ¹ì§•: ì´ˆê¸° SOC ëœë¤í™”, ë©”ëª¨ë¦¬ ê´€ë¦¬, ìë™ ì €ì¥

Simulink.sdi.clear; % â˜… ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€ (ë°ì´í„° ìºì‹œ ì‚­ì œ)

fprintf('===========================================\n');
fprintf('   ğŸš€ EREV RL í•™ìŠµ ì‹œì‘ (Final Run)\n');
fprintf('===========================================\n\n');

%% 1. ì¤€ë¹„
if ~exist('EREV_RL_agent.mat', 'file')
    error('âŒ Agent íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. create_EREV_agent.mì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.');
end
load('EREV_RL_agent.mat');

modelName = 'EREV_1_Model_RL';
agentBlock = 'EREV_1_Model_RL/RL_Agent';

if ~bdIsLoaded(modelName)
    load_system(modelName);
end

%% 2. í™˜ê²½ ìƒì„± & ë¦¬ì…‹ í•¨ìˆ˜ ì—°ê²°
env = rlSimulinkEnv(modelName, agentBlock);
env.ResetFcn = @myResetFunction; % í•˜ë‹¨ í•¨ìˆ˜ ì°¸ì¡°

%% 3. Training Options (ì•ˆì „ ì œì¼)
trainOpts = rlTrainingOptions(...
    'MaxEpisodes', 1000, ...
    'MaxStepsPerEpisode', 30000, ...
    'ScoreAveragingWindowLength', 20, ...
    'Verbose', true, ...
    'Plots', 'training-progress', ...
    'StopTrainingCriteria', 'AverageReward', ...
    'StopTrainingValue', 10000, ... 
    'SaveAgentCriteria', 'EpisodeCount', ... % ì£¼ê¸°ì  ì €ì¥
    'SaveAgentValue', 50, ...                % 50íŒë§ˆë‹¤ ì €ì¥
    'SaveAgentDirectory', 'saved_agents');

%% 4. í•™ìŠµ ì‹¤í–‰
tic;
try
    trainingStats = train(agent, env, trainOpts);
    
    % ì¢…ë£Œ í›„ ì €ì¥
    timestamp = datestr(now, 'yyyymmdd_HHMMSS');
    save(sprintf('Final_Agent_%s.mat', timestamp), 'agent', 'trainingStats');
    fprintf('\nâœ… í•™ìŠµ ì™„ë£Œ! ì €ì¥ë¨.\n');
    
catch ME
    fprintf('\nâŒ ì˜¤ë¥˜ ë°œìƒ: %s\n', ME.message);
end

%% [ë¶€ë¡] ì´ˆê¸°í™” í•¨ìˆ˜
function in = myResetFunction(in)
    % SOCë¥¼ 25% ~ 75% ì‚¬ì´ ëœë¤ ì„¤ì •
    init_soc = 0.25 + 0.5 * rand();
    in = setVariable(in, 'SOC_init', init_soc);
end